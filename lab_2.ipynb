{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otlish/Lab1_DWDM/blob/main/lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "052FVHsUVf_A",
        "outputId": "2915fbe0-66b5-4c9c-a8bd-d25a1b33e1a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.11/dist-packages (0.23.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mlxtend pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
        "import time\n",
        "\n",
        "file_paths = {\n",
        "    'space.txt': '/content/drive/MyDrive/data_mining_and_warehousing/space.txt',\n",
        "    'sports.txt': '/content/drive/MyDrive/data_mining_and_warehousing/sports.txt'\n",
        "}\n",
        "min_support = 0.15\n",
        "min_confidence = 0.7\n",
        "\n",
        "def get_support(itemset, transactions):\n",
        "    count = sum(1 for tx in transactions if itemset.issubset(set(tx)))\n",
        "    return count / len(transactions)\n",
        "\n",
        "def apriori(transactions, min_support):\n",
        "    total_tx = len(transactions)\n",
        "    item_counts = defaultdict(int)\n",
        "\n",
        "    for tx in transactions:\n",
        "        for item in tx:\n",
        "            item_counts[frozenset([item])] += 1\n",
        "\n",
        "    frequent_itemsets = {item: count for item, count in item_counts.items() if count / total_tx >= min_support}\n",
        "    all_frequent = frequent_itemsets.copy()\n",
        "    current_freq = list(frequent_itemsets.keys())\n",
        "    k = 2\n",
        "\n",
        "    while current_freq:\n",
        "        candidates = set()\n",
        "        for i in range(len(current_freq)):\n",
        "            for j in range(i + 1, len(current_freq)):\n",
        "                union = current_freq[i] | current_freq[j]\n",
        "                if len(union) == k:\n",
        "                    candidates.add(union)\n",
        "\n",
        "        candidate_counts = defaultdict(int)\n",
        "        for tx in transactions:\n",
        "            tx_set = set(tx)\n",
        "            for candidate in candidates:\n",
        "                if candidate.issubset(tx_set):\n",
        "                    candidate_counts[candidate] += 1\n",
        "\n",
        "        current_freq = [item for item in candidate_counts if candidate_counts[item] / total_tx >= min_support]\n",
        "        all_frequent.update({item: candidate_counts[item] for item in current_freq})\n",
        "        k += 1\n",
        "\n",
        "    return all_frequent\n",
        "\n",
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    total_tx = len(transactions)\n",
        "    rules = []\n",
        "    for itemset in frequent_itemsets:\n",
        "        if len(itemset) < 2:\n",
        "            continue\n",
        "        support_itemset = frequent_itemsets[itemset] / total_tx\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in combinations(itemset, i):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "                support_ante = get_support(antecedent, transactions)\n",
        "                support_cons = get_support(consequent, transactions)\n",
        "                confidence = support_itemset / support_ante\n",
        "                lift = confidence / support_cons\n",
        "                if confidence >= min_confidence:\n",
        "                    rules.append({\n",
        "                        'antecedents': set(antecedent),\n",
        "                        'consequents': set(consequent),\n",
        "                        'support': round(support_itemset, 2),\n",
        "                        'confidence': round(confidence, 2),\n",
        "                        'lift': round(lift, 2)\n",
        "                    })\n",
        "    return rules\n",
        "\n",
        "for name, path in file_paths.items():\n",
        "    print(f\"\\n===== Processing {name} =====\")\n",
        "\n",
        "    transactions = []\n",
        "    with open(path, 'r') as file:\n",
        "        next(file)\n",
        "        for line in file:\n",
        "            parts = line.strip().split(',')\n",
        "            transactions.append([item.strip() for item in parts[1:] if item.strip()])\n",
        "\n",
        "    frequent_itemsets_raw = apriori(transactions, min_support)\n",
        "    rules = generate_rules(frequent_itemsets_raw, transactions, min_confidence)\n",
        "\n",
        "    total_tx = len(transactions)\n",
        "    frequent_itemsets_df = pd.DataFrame([{\n",
        "        'itemsets': set(item),\n",
        "        'support': round(count / total_tx, 2)\n",
        "    } for item, count in frequent_itemsets_raw.items()])\n",
        "\n",
        "    rules_df = pd.DataFrame(rules)\n",
        "\n",
        "    print(\"\\nFrequent Itemsets:\\n\", frequent_itemsets_df)\n",
        "\n",
        "    if not rules_df.empty:\n",
        "        print(\"\\nAssociation Rules:\\n\", rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "    else:\n",
        "        print(\"\\nNo association rules found with confidence ≥\", min_confidence)\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df_fp = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "fp_itemsets = fpgrowth(df_fp, min_support=min_support, use_colnames=True)\n",
        "\n",
        "fp_rules = association_rules(fp_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "\n",
        "print(\"\\nFP-Growth Frequent Itemsets:\\n\", fp_itemsets)\n",
        "if not fp_rules.empty:\n",
        "    print(\"\\nFP-Growth Association Rules:\\n\", fp_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "else:\n",
        "    print(\"\\nNo association rules found using FP-Growth with confidence ≥\", min_confidence)\n",
        "\n",
        "\n",
        "start_apriori = time.time()\n",
        "frequent_itemsets_raw = apriori(transactions, min_support)\n",
        "rules = generate_rules(frequent_itemsets_raw, transactions, min_confidence)\n",
        "end_apriori = time.time()\n",
        "\n",
        "start_fp = time.time()\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df_fp = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "fp_itemsets = fpgrowth(df_fp, min_support=min_support, use_colnames=True)\n",
        "fp_rules = association_rules(fp_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "end_fp = time.time()\n",
        "\n",
        "print(f\"\\nExecution Time (Apriori): {round(end_apriori - start_apriori, 4)} seconds\")\n",
        "print(f\"Execution Time (FP-Growth): {round(end_fp - start_fp, 4)} seconds\")\n",
        "\n",
        "\n",
        "print(\"\\n=== Comparison Summary ===\")\n",
        "print(f\"Apriori generated {len(rules)} rules\")\n",
        "print(f\"FP-Growth generated {len(fp_rules)} rules\")\n",
        "if (end_apriori - start_apriori) > (end_fp - start_fp):\n",
        "    print(\"FP-Growth is faster than Apriori.\")\n",
        "else:\n",
        "    print(\"Apriori is faster than FP-Growth.\")\n",
        "print(\"Both algorithms generated similar types of association rules, but FP-Growth is generally more efficient for large datasets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3nSxSWhcW7p",
        "outputId": "aa8d0dbb-d65f-4d2d-a49a-aad5d840389e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Processing space.txt =====\n",
            "\n",
            "Frequent Itemsets:\n",
            "                      itemsets  support\n",
            "0               {Robotic Arm}     0.34\n",
            "1              {Food Packets}     0.40\n",
            "2              {Sleeping Bag}     0.32\n",
            "3                 {Treadmill}     0.28\n",
            "4                {Space Suit}     0.32\n",
            "5                {3D Printer}     0.28\n",
            "6  {Carbon Dioxide Scrubbers}     0.24\n",
            "\n",
            "No association rules found with confidence ≥ 0.7\n",
            "\n",
            "===== Processing sports.txt =====\n",
            "\n",
            "Frequent Itemsets:\n",
            "          itemsets  support\n",
            "0      {football}     0.44\n",
            "1  {cricket ball}     0.36\n",
            "2        {gloves}     0.36\n",
            "3   {cricket bat}     0.40\n",
            "4         {juice}     0.42\n",
            "5  {water bottle}     0.28\n",
            "6     {ice cream}     0.26\n",
            "\n",
            "No association rules found with confidence ≥ 0.7\n",
            "\n",
            "FP-Growth Frequent Itemsets:\n",
            "    support        itemsets\n",
            "0     0.44      (football)\n",
            "1     0.36        (gloves)\n",
            "2     0.36  (cricket ball)\n",
            "3     0.42         (juice)\n",
            "4     0.40   (cricket bat)\n",
            "5     0.28  (water bottle)\n",
            "6     0.26     (ice cream)\n",
            "\n",
            "No association rules found using FP-Growth with confidence ≥ 0.7\n",
            "\n",
            "Execution Time (Apriori): 0.0002 seconds\n",
            "Execution Time (FP-Growth): 0.0034 seconds\n",
            "\n",
            "=== Comparison Summary ===\n",
            "Apriori generated 0 rules\n",
            "FP-Growth generated 0 rules\n",
            "Apriori is faster than FP-Growth.\n",
            "Both algorithms generated similar types of association rules, but FP-Growth is generally more efficient for large datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-Nx0UDJdZ3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}